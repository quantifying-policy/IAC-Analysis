{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2a. Pre-Processing: Organisation Aliases\n",
    "\n",
    "As mentioned in the previous step, there is a serious issue with organisation names. They are not uniformly applied, such that there are several entries for the same organisation e.g., \"ESA\", \" ESA\", \"European Space Agency\", \"European Space Agency (ESA)\". Making this data as uniform as possible is important, because this way we'll have reliable data on the involvement of different organisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667460592124,
     "user": {
      "displayName": "Daniel Leichte",
      "userId": "15787498902779762448"
     },
     "user_tz": -60
    },
    "id": "HsncQbseEK54"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import re\n",
    "import json\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIU54loPGCZ-"
   },
   "source": [
    "Unpickling the raw data from step 1 as dict_of_info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vnfcc9K8F1mH"
   },
   "outputs": [],
   "source": [
    "directory = \"/content/drive/MyDrive/Colab Notebooks/ESPI_Codes/IAC_Analysis/1.HTML_Parsing/\"\n",
    "with open(directory+\"IAC_raw_data.pickle\", \"rb\") as handle:\n",
    "  input = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tjs-JL4aInIj"
   },
   "source": [
    "## 1 . Simplified dict\n",
    "This will only contain as key the paper_id and as value the organisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jW6MXmzH_60"
   },
   "outputs": [],
   "source": [
    "raw1 = {}\n",
    "for key, value in input.items():\n",
    "  for id, info in value.items():\n",
    "    raw1.update({id: info[\"company\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2s_cubTrJFjT"
   },
   "source": [
    "## 2. Separating the organisation names\n",
    "In some cases, multiple authors from different organiations authored a paper. Their names are one string separated by semicolons. This code will separate them and put them in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xqfka31GJYBh"
   },
   "outputs": [],
   "source": [
    "raw2 = {}\n",
    "for key, value in raw1.items():\n",
    "  raw2.update({key: value.split(\";\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6qPeKtuNVFB3"
   },
   "source": [
    "## 3. Removing empty spaces in organisation names\n",
    "Some organisation names have an empty space in the beginning or end (or both). This codes removes them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6SlvQ10UUT4"
   },
   "outputs": [],
   "source": [
    "raw3 = {}\n",
    "for key, value in raw2.items():\n",
    "  raw3.update({key: []})\n",
    "  for element in value:\n",
    "    if len(element) > 2 and element != \"N/A\":    \n",
    "      if element[0] is \" \" and element[-1] is \" \":\n",
    "        raw3[key].append(element[1:-1])\n",
    "      elif element[0] is \" \" and element[-1] is not \" \":\n",
    "        raw3[key].append(element[1:])\n",
    "      elif element[-1] is \" \" and element[0] is not \" \":\n",
    "        raw3[key].append(element[:-1])\n",
    "      else:\n",
    "        raw3[key].append(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBYVwZu9VQJP"
   },
   "source": [
    "## 4. Harmonising organisation names\n",
    "Many organisations have several aliases, partly due to mispellings or including/excluding the abbreviation. I've compiled a dict of organisations that appeared particularly often (synonyms1) along with a list of aliases as the value. Using regex, we'll iterate over the data; if any of the aliases is mentioned as the organisation name, we'll replace it with the keys of synonyms1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ytWfsHN4l0l9"
   },
   "outputs": [],
   "source": [
    "synonyms1 = {\"ESA\": [\"ESA\", \"European Space Agency\", \"ESTEC\"],\n",
    "                 \"DLR\": [\"DLR\", \"German Aerospace Center\", \"Zentrum fr Luft- und Raumfahrt\", \"Zentrum fuer Luft- und Raumfahrt\"],\n",
    "                 \"CNES\": [\"CNES\", \"Centre National d'Etudes Spatiales\"],\n",
    "                 \"ASI\": [\"ASI\", \"Italian Space Agency\", \"Agenzia Spaziale Italiana\"],\n",
    "                 \"NASA\": [\"NASA\", \"National Aeronautics and Space Administration\"],\n",
    "                 \"ISRO\": [\"ISRO\", \"Indian Space Research Organization\"],\n",
    "                 \"KARI\": [\"KARI\", \"Korea Aerospace Research Institute\"],\n",
    "                 \"EUSPA\": [\"EUSPA\", \"European Union Agency for the Space Programme\"],\n",
    "                 \"ROSCOSMOS\": [\"ROSCOSMOS\", \"Federal Space Agency (ROSCOSMOS)\"],\n",
    "                 \"JAXA\": [\"JAXA\", \"Japan Aerospace Exploration Agency\"],\n",
    "                 \"UKSA\": [\"UKSA\", \"UK Space Agency\"],\n",
    "                 \"POLSA\": [\"POLSA\", \"Polish Space Agency\"],\n",
    "                 \"OHB\": [\"OHB\", \"OHB System\"],\n",
    "                 \"Airbus\": [\"Airbus\"],\n",
    "                 \"Thales\": [\"Thales Alenia Space\", \"Thales\"],\n",
    "             \"Telespazio\": [\"Telespazio\"]\n",
    "                 }\n",
    "synonyms1_list = []\n",
    "for key, value in synonyms1.items():\n",
    "  for word in value:\n",
    "    synonyms1_list.append(word)\n",
    "\n",
    "raw4 = {}\n",
    "for key, value in raw3.items():\n",
    "  raw4.update({key: []})\n",
    "  for element in value:\n",
    "    if re.search(\"|\".join(synonyms1_list), element):\n",
    "      for orga, list_of_aliases in synonyms1.items():\n",
    "        if re.search(\"|\".join(list_of_aliases), element):\n",
    "          raw4[key].append(synonyms1[orga][0])\n",
    "    elif re.search(\"|\".join(synonyms1), element) is None:\n",
    "      raw4[key].append(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qfbAzIlwzgGX"
   },
   "source": [
    "## 5. Fixing Spelling Mistakes\n",
    "Many people misspelled university:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r5RWElYLoYyw"
   },
   "outputs": [],
   "source": [
    "raw5 = {}\n",
    "regex = \"Un[\\S]*ty\"\n",
    "regex2 = \"Univeristy\"\n",
    "for key, value in raw4.items():\n",
    "  raw5.update({key: []})\n",
    "  for element in value:\n",
    "    if re.search(regex, element):\n",
    "      new = re.sub(regex, \"University\", element)\n",
    "      raw5[key].append(new)\n",
    "    elif re.search(\"Uiniversity\", element):\n",
    "      new = re.sub(\"Uiniversity\", \"University\", element)\n",
    "      raw5[key].append(new)\n",
    "    else:\n",
    "      raw5[key].append(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uN_WxD44d2_9"
   },
   "source": [
    "## 6. Removing multiple mentions of the same organisation\n",
    "When multiple authors from the same organisation wrote an abstract, their organisation name is often repeated. To avoid double-counting later on, I'll remove them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDIOJ7xBd3sm"
   },
   "outputs": [],
   "source": [
    "raw6 = {}\n",
    "for key, value in raw5.items():\n",
    "  raw6.update({key: []})\n",
    "  for element in value:\n",
    "    if element not in raw6[key]:\n",
    "        raw6[key].append(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAOHRevJ4KJq"
   },
   "source": [
    "## 7. Looking for more Aliases\n",
    "In step 4 I created a list of organisation-aliases that are well known to me. The problem is very pervasive though, with \"TU MÃ¼nchen\" also written as \"TU Munich\". I use SequenceMatcher to compare the similarity of all organisation names with one another. If there is a high similarity, it is likely that they are spelling-variants of the same organisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kjtHo3yc5gOP"
   },
   "outputs": [],
   "source": [
    "#creating a list of unique organisation names\n",
    "to_check = []\n",
    "for key, value in raw6.items():\n",
    "  for orga in value:\n",
    "    if orga not in to_check:\n",
    "      to_check.append(orga)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZAvlpo64Kfq"
   },
   "outputs": [],
   "source": [
    "#iterating over the list from above to check for similar organisation names\n",
    "result1 = {}\n",
    "checked = []\n",
    "iteration = 1\n",
    "\n",
    "for orga in to_check:\n",
    "  if orga not in checked:\n",
    "    checked.append(orga)\n",
    "    one = orga\n",
    "    result1.update({one: [one]})\n",
    "    for orga in to_check[iteration:]:\n",
    "      two = orga\n",
    "      sim = SequenceMatcher(None, one, two).ratio()\n",
    "      if sim >= 0.9 and sim != 1 and two not in checked:\n",
    "        result1[one].append(two)\n",
    "        checked.append(two)\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zp_vDxbD5Zcn"
   },
   "outputs": [],
   "source": [
    "#filtering out lists that have at least one alias\n",
    "result2 = {}\n",
    "for key, value in result1.items():\n",
    "  if len(value) > 1:\n",
    "    result2.update({key: value})\n",
    "\n",
    "#As a rule of thumb, the alias with the longest name is probably best as it entails the abbreviation\n",
    "result3 = {}\n",
    "for key, value in result2.items():\n",
    "  tmp = sorted(value, key = len)\n",
    "  result3.update({tmp[-1]: tmp})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BhTwkknGvdT_"
   },
   "source": [
    "This works fairly well, but it's not perfect. E.g., \"University Bologna\" matched with \"University Bonn\". In other words, I have to manually double check and delete the mistakes. I did this in another notebook and here I'm loading the results. It's a dictionary like synonyms1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 769,
     "status": "ok",
     "timestamp": 1667460598194,
     "user": {
      "displayName": "Daniel Leichte",
      "userId": "15787498902779762448"
     },
     "user_tz": -60
    },
    "id": "YVD_1TrE8XcT"
   },
   "outputs": [],
   "source": [
    "directory = \"/content/drive/MyDrive/Colab Notebooks/ESPI_Codes/IAC_Analysis/2.Pre-Processing/\"\n",
    "with open(directory+\"2ai.synonyms2.pickle\", \"rb\") as handle:\n",
    "  synonyms2 = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PsIZxRp-xXN2"
   },
   "outputs": [],
   "source": [
    "synonyms2_list = []\n",
    "for key, value in synonyms2.items():\n",
    "  for word in value:\n",
    "    synonyms2_list.append(word)\n",
    "\n",
    "raw7 = {}\n",
    "for key, value in raw6.items():\n",
    "  raw7.update({key: []})\n",
    "  for element in value:\n",
    "    if re.search(\"|\".join(synonyms2_list), element):\n",
    "      for orga, list_of_aliases in synonyms2.items():\n",
    "        if re.search(\"|\".join(list_of_aliases), element):\n",
    "          raw7[key].append(synonyms2[orga][-1])\n",
    "    elif re.search(\"|\".join(synonyms2), element) is None:\n",
    "      raw7[key].append(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SffKOpOs3Kty"
   },
   "source": [
    "Removing the \\\\ in front of ( and ) from the previous step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ej04-pxB3SiW"
   },
   "outputs": [],
   "source": [
    "raw8 = {}\n",
    "for key, value in raw7.items():\n",
    "  raw8.update({key: []})\n",
    "  for element in value:\n",
    "    tmp = re.sub(re.escape(\"\\\\\"), \"\", element)\n",
    "    raw8[key].append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rP7xfpLS6BAm"
   },
   "source": [
    "Removing multiple mentions of the same organisation (for the second time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TwZXv4M6HIp"
   },
   "outputs": [],
   "source": [
    "raw9 = {}\n",
    "for key, value in raw8.items():\n",
    "  raw9.update({key: []})\n",
    "  for element in value:\n",
    "    if element not in raw9[key]:\n",
    "      raw9[key].append(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2U1lZgU7CuL"
   },
   "source": [
    "## 8. Conclusion\n",
    "\n",
    "How many organisations have we been able to consolidate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1079,
     "status": "ok",
     "timestamp": 1667335317756,
     "user": {
      "displayName": "Daniel Leichte",
      "userId": "15787498902779762448"
     },
     "user_tz": -60
    },
    "id": "4ZtyUyEe6mbn",
    "outputId": "b8ae7b31-181c-4dc4-b53c-ec317a1ab32b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Originally: 4836 | After cleaning: 2947 | Reduced by: 1889.\n"
     ]
    }
   ],
   "source": [
    "unique1 = []\n",
    "for key, value in raw2.items():\n",
    "  for element in value:\n",
    "    if element not in unique1:\n",
    "      unique1.append(element)\n",
    "\n",
    "unique2 = []\n",
    "for key, value in raw9.items():\n",
    "  for element in value:\n",
    "    if element not in unique2:\n",
    "      unique2.append(element)\n",
    "\n",
    "print(f\"Originally: {len(unique1)} | After cleaning: {len(unique2)} | Reduced by: {len(unique1)-len(unique2)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, there are probably still many aliases there, but we've significantly increased the quality of the data. In the next step (Pre-Processing 2b), we'll categorise the organisations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPxqwxoo7qh1"
   },
   "source": [
    "## 9. Exporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WiG4jQFL7tjr"
   },
   "outputs": [],
   "source": [
    "# with open(\"2a.cleaned_organisation_names.pickle\", \"wb\") as handle:\n",
    "#   pickle.dump(raw9, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyO4rkOyM+GMDEe2bC8RBRIQ",
   "mount_file_id": "1p0Hf1rE9WVBi0HP2BjAe5QiR4q_yRdwU",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
